{% import 'config/cluster_read_library.yaml' as cluster_read_library %}

# Cluster resources related metrics
- name: measurements.storage_count_attachable_volumes_in_use
  monitoring_query: sum(storage_count_attachable_volumes_in_use)
  monitoring_step: 15

- name: measurements.cluster_cpu_usage_seconds_total_rate
  monitoring_query: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{cluster=""})
  monitoring_step: 15

- name: measurements.workers_avg_cpu_usage_percentage
  monitoring_query: |
    sum(
      rate(
        node_cpu_seconds_total{mode!="idle"}[5m]
      )
      *
      on(instance) group_left(role) label_replace(
        kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"
      )
    )
    /
    count(
      node_cpu_seconds_total{mode="idle"}
      *
      on(instance) group_left(role) label_replace(
        kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"
      )
    )
    *
    100
  monitoring_step: 15

- name: measurements.cluster_memory_usage_rss_total
  monitoring_query: sum(container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", cluster="", container!=""})
  monitoring_step: 15

- name: measurements.cluster_disk_throughput_total
  monitoring_query: sum (rate(container_fs_reads_bytes_total{id!="", device=~"(/dev.+)|mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+", cluster=""}[5m]) + rate(container_fs_writes_bytes_total{id!="", device=~"(/dev.+)|mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+", cluster=""}[5m]))
  monitoring_step: 15

- name: measurements.cluster_nodes_worker_count
  monitoring_query: count(kube_node_role{role="worker"})
  monitoring_step: 15

- name: measurements.cluster_pods_count
  monitoring_query: count(kube_pod_info)
  monitoring_step: 15

- name: measurements.cluster_running_pods_on_workers_count
  monitoring_query: count(kube_pod_info * on(node) group_left(role) kube_node_role{role="worker"} and on(pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"} > 0))
  monitoring_step: 15

- name: measurements.scheduler_pending_pods_count
  monitoring_query: sum(scheduler_pending_pods)
  monitoring_step: 15

- name: measurements.cluster_network_bytes_total
  monitoring_query: sum(irate(container_network_receive_bytes_total{cluster="",namespace=~".*"}[5m])) + sum(irate(container_network_transmit_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.cluster_network_receive_bytes_total
  monitoring_query: sum(irate(container_network_receive_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.cluster_network_transmit_bytes_total
  monitoring_query: sum(irate(container_network_transmit_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.node_disk_io_time_seconds_total
  monitoring_query: sum(irate(node_disk_io_time_seconds_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15



# Tekton related metrics
- name: measurements.tekton_pipelines_controller_running_pipelineruns_count
  monitoring_query: sum(tekton_pipelines_controller_running_pipelineruns_count)
  monitoring_step: 15

- name: measurements.tekton_tekton_pipelines_controller_workqueue_depth
  monitoring_query: sum(tekton_pipelines_controller_workqueue_depth)
  monitoring_step: 15

- name: measurements.tekton_tekton_chains_controller_workqueue_depth
  monitoring_query: sum(watcher_workqueue_depth{container='tekton-chains-controller'})
  monitoring_step: 15

###- name: measurements.pipelinerun_duration_scheduled_seconds
###  monitoring_query: sum(pipelinerun_duration_scheduled_seconds_sum / pipelinerun_duration_scheduled_seconds_count)
###  monitoring_step: 15

- name: measurements.tekton_pipelines_controller_running_taskruns_throttled_by_node
  monitoring_query: sum(tekton_pipelines_controller_running_taskruns_throttled_by_node_count)
  monitoring_step: 15

- name: measurements.tekton_pipelines_controller_running_taskruns_throttled_by_quota
  monitoring_query: sum(tekton_pipelines_controller_running_taskruns_throttled_by_quota_count)
  monitoring_step: 15

- name: measurements.tekton_pipelines_controller_client_latency_average
  monitoring_query: sum(rate(tekton_pipelines_controller_client_latency_sum[1m]) / rate(tekton_pipelines_controller_client_latency_count[1m]))
  monitoring_step: 15

- name: measurements.tekton_pipelines_controller_taskruns_pod_latency_milliseconds
  monitoring_query: sum(tekton_pipelines_controller_taskruns_pod_latency_milliseconds)
  monitoring_step: 15



# Cluster API server related metrics
- name: measurements.apiserver_request_total_rate
  monitoring_query: sum(rate(apiserver_request_total{}[5m]))
  monitoring_step: 15

- name: measurements.etcd_request_duration_seconds_average
  monitoring_query: sum(rate(etcd_request_duration_seconds_sum{}[5m])) / sum(rate(etcd_request_duration_seconds_count[5m]))
  monitoring_step: 15

- name: measurements.etcd_mvcc_db_total_size_in_bytes_average
  monitoring_query: avg(etcd_mvcc_db_total_size_in_bytes)
  monitoring_step: 15

- name: measurements.etcd_mvcc_db_total_size_in_use_in_bytes_average
  monitoring_query: avg(etcd_mvcc_db_total_size_in_use_in_bytes)
  monitoring_step: 15

- name: measurements.etcd_server_quota_backend_bytes_average
  monitoring_query: avg(etcd_server_quota_backend_bytes)
  monitoring_step: 15

# Interesting CI environment variables
{% for var in [
  'BUILD_ID',
  'DEPLOYMENT_TYPE',
  'DEPLOYMENT_VERSION',
  'DEPLOYMENT_PIPELINES_CONTROLLER_RESOURCES',
  'DEPLOYMENT_PIPELINES_CONTROLLER_HA_REPLICAS',
  'DEPLOYMENT_CHAINS_CONTROLLER_HA_REPLICAS',
  'HOSTNAME',
  'JOB_NAME',
  'OPENSHIFT_API',
  'PROW_JOB_ID',
  'PULL_BASE_REF',
  'PULL_BASE_SHA',
  'PULL_HEAD_REF',
  'PULL_NUMBER',
  'PULL_PULL_SHA',
  'PULL_REFS',
  'REPO_NAME',
  'REPO_OWNER',
  'TEST_CONCURRENT',
  'TEST_SCENARIO',
  'TEST_TOTAL',
  'TEST_DO_CLEANUP',
] %}
- name: metadata.env.{{ var }}
  env_variable: {{ var }}
{% endfor %}



# Gather some basic info about the cluster
- name: metadata.cluster.context
  command: oc project default > /dev/null && oc config current-context

- name: metadata.cluster.control-plane.count
  command: oc get nodes -l node-role.kubernetes.io/master -o name | wc -l

- name: metadata.cluster.control-plane.flavor
  command: oc get nodes -l node-role.kubernetes.io/master -o json | jq --raw-output '.items | map(.metadata.labels."beta.kubernetes.io/instance-type") | unique | sort | join(",")'

- name: metadata.cluster.control-plane.nodes
  command: oc get nodes -l node-role.kubernetes.io/master -o json | jq '.items | map(.metadata.name)'
  output: json

- name: metadata.cluster.compute-nodes.count
  command: oc get nodes -l node-role.kubernetes.io/worker -o name | wc -l

- name: metadata.cluster.compute-nodes.flavor
  command: oc get nodes -l node-role.kubernetes.io/worker -o json | jq --raw-output '.items | map(.metadata.labels."beta.kubernetes.io/instance-type") | unique | sort | join(",")'

- name: metadata.cluster.compute-nodes.nodes
  command: oc get nodes -l node-role.kubernetes.io/worker -o json | jq '.items | map(.metadata.name)'
  output: json



# Collect versions of various components
{% if DEPLOYMENT_TYPE == 'downstream' %}
- name: parameters.cluster.pod.openshift-pipelines-operator.version
  command: "oc -n openshift-operators get pods -l app=openshift-pipelines-operator -o json | jq '[ .items[0] | .spec.containers[] | {\"key\": .name, \"value\": .env | map(select(.name == \"VERSION\"))[0].value} ] | from_entries'"
  output: json
{% endif %}

- name: parameters.cluster.pod.openshift-pipelines-configmaps.version
  command: "oc -n {% if DEPLOYMENT_TYPE == 'downstream' %}openshift-pipelines{% elif DEPLOYMENT_TYPE == 'upstream' %}tekton-pipelines{% else %}NA{% endif %} get configmap -o json | jq '[ .items[] | if select(.data.version != null) then {\"key\": .metadata.name, \"value\": .data.version} else {} end ] | from_entries'"
  output: json


# Collect data for relevant pods
{% if DEPLOYMENT_TYPE == 'downstream' %}

{% set deployments = ['pipelines-as-code-controller','pipelines-as-code-watcher','pipelines-as-code-webhook','tekton-chains-controller','tekton-events-controller','tekton-operator-proxy-webhook','tekton-pipelines-controller','tekton-pipelines-remote-resolvers','tekton-pipelines-webhook','tekton-triggers-controller','tekton-triggers-core-interceptors','tekton-triggers-webhook','tkn-cli-serve'] %}
{% set containers = ['pac-controller','pac-watcher','pac-webhook','tekton-chains-controller','tekton-events-controller','proxy','tekton-pipelines-controller','controller','webhook','tekton-triggers-controller','tekton-triggers-core-interceptors','webhook','tkn-cli-serve'] %}
{% set namespace = 'openshift-pipelines' %}

{% elif DEPLOYMENT_TYPE == 'upstream' %}

{% set deployments = ['tekton-events-controller','tekton-pipelines-controller','tekton-pipelines-webhook','tekton-chains-controller'] %}
{% set containers = ['tekton-events-controller','tekton-pipelines-controller','webhook','tekton-chains-controller'] %}
{% set namespace = 'tekton-pipelines' %}

{% else %}

# Nothing

{% endif %}

{% for item in deployments %}
{{ cluster_read_library.monitor_pod(namespace, deployments[loop.index0], 15) }}
{{ cluster_read_library.pod_info(namespace, deployments[loop.index0], containers[loop.index0]) }}
{% endfor %}



# Collect data for API pods
{{ cluster_read_library.monitor_pod('openshift-apiserver', 'apiserver', 15) }}
{{ cluster_read_library.monitor_pod('openshift-kube-apiserver', 'kube-apiserver', 15, pod_suffix_regex='-ip-.+') }}
{{ cluster_read_library.monitor_pod('openshift-etcd', 'etcd', 15, pod_suffix_regex='-ip-.+') }}



# Collect data for Tekton Results Pods
{{ cluster_read_library.monitor_pod('openshift-pipelines', 'tekton-results-api', 15) }}
{{ cluster_read_library.monitor_pod('openshift-pipelines', 'tekton-results-watcher', 15) }}
{{ cluster_read_library.monitor_pod('tekton-pipelines', 'tekton-results-api', 15) }}
{{ cluster_read_library.monitor_pod('tekton-pipelines', 'tekton-results-watcher', 15) }}


- name: results.locust_requests_fail_ratio
  monitoring_query: locust_requests_fail_ratio
  monitoring_step: 15
- name: results.locust_users
  monitoring_query: locust_users
  monitoring_step: 15

{{ cluster_read_library.results_scenario('/log') }}
{{ cluster_read_library.results_scenario('/record') }}
{{ cluster_read_library.results_scenario('/records') }}


# Collect metrics related to logging
## Rate metrics
- name: measurements.logs.collection_rate # Bytes per second (Bps)
  monitoring_query: sum(rate(vector_component_received_bytes_total{component_kind="source", component_type!="internal_metrics"}[5m]))
  monitoring_step: 15
- name: measurements.logs.sent_rate # Bytes per second (Bps)
  monitoring_query: sum(rate(vector_component_sent_bytes_total{component_kind="sink", component_type!="prometheus_exporter"}[5m]))
  monitoring_step: 15

## Resource Usage
- name: measurements.logs.collector.cpu # No. of CPU cores
  monitoring_query: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container='collector'})
  monitoring_step: 15
- name: measurements.logs.collector.memory # Bytes
  monitoring_query: sum(node_namespace_pod_container:container_memory_rss{container="collector"})
  monitoring_step: 15

## File System Usage
- name: measurements.logs.open_files
  monitoring_query: sum(vector_open_files{component_kind="source", component_type="kubernetes_logs"})
  monitoring_step: 15

## Loki metrics
- name: measurements.logs.loki.events_sent
  monitoring_query: sum(vector_component_sent_events_total{component_kind='sink', component_type='loki'})
  monitoring_step: 15
- name: measurements.logs.loki.event_bytes_sent # Bytes
  monitoring_query: sum(vector_component_sent_event_bytes_total{component_kind='sink', component_type='loki'}) 
  monitoring_step: 15
- name: measurements.logs.loki.distributor_lines_received
  monitoring_query: sum(loki_distributor_lines_received_total)
  monitoring_step: 15
- name: measurements.logs.loki.distributor_ingester_appends
  monitoring_query: sum(loki_distributor_ingester_appends_total)
  monitoring_step: 15
- name: measurements.logs.loki.ingester_records_logged_rate
  monitoring_query: sum(rate(loki_ingester_wal_records_logged_total[5m]))
  monitoring_step: 15
- name: measurements.logs.loki.internal_error_log_count
  monitoring_query: sum(loki_internal_log_messages_total{level='error'})
  monitoring_step: 15
- name: measurements.logs.loki.internal_debug_log_count
  monitoring_query: sum(loki_internal_log_messages_total{level='debug'})
  monitoring_step: 15
- name: measurements.logs.loki.internal_info_log_count
  monitoring_query: sum(loki_internal_log_messages_total{level='info'})
  monitoring_step: 15
- name: measurements.logs.loki.internal_warn_log_count
  monitoring_query: sum(loki_internal_log_messages_total{level='warn'})
  monitoring_step: 15
